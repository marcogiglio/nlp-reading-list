
# List of important papers

## General
- [Scaling Laws for Neural Language Models](https://arxiv.org/abs/2001.08361): a study of the performance curves of auto-regressive neural language models at different parameters sizes 


## Transformers Architecture
- [Attention is all you need](https://arxiv.org/abs/1706.03762):
- [Reformer](https://arxiv.org/abs/2001.04451): 
- [Linformer](https://arxiv.org/abs/2006.04768): 

## Models
- [Language Models are Few-Shot Learners](https://arxiv.org/abs/2005.14165): the GPT-3 paper, a massive autoregressive language models with 170 billions parameters. 
