
# List of important papers

## General
- [Scaling Laws for Neural Language Models](https://arxiv.org/abs/2001.08361): a study of the performance curves of auto-regressive neural language models at different parameters sizes 
- [Neural Machine Translation of Rare Words with Subword Units](https://www.aclweb.org/anthology/P16-1162.pdf): the paper is the first to apply the Byte-Pair-Encoding algorithm to encode words as sequences of subwords units. This enables Neural Machine Translation to tackle open-words vocabulary.


## Transformers Architecture
- [Attention is all you need](https://arxiv.org/abs/1706.03762): the original Transformer architecture paper, presented as a sequence to sequence encoder-decoder model. 
- [Reformer](https://arxiv.org/abs/2001.04451): a more efficient transformer 
- [Linformer](https://arxiv.org/abs/2006.04768): a linear approximation of the self-attention mechanism 

## Models
- [Language Models are Few-Shot Learners](https://arxiv.org/abs/2005.14165): the GPT-3 paper, a massive autoregressive language model with 170 billions parameters. 
