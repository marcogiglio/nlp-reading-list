
# List of important papers

## General
- [Scaling Laws for Neural Language Models](https://arxiv.org/abs/2001.08361): a study of the performance curves of auto-regressive neural language models at different parameters sizes 


## Transformers Architecture
- [Attention is all you need](https://arxiv.org/abs/1706.03762): the original Transformer architecture paper, presented as a sequence to sequence encoder-decoder model. 
- [Reformer](https://arxiv.org/abs/2001.04451): a more efficient transformer 
- [Linformer](https://arxiv.org/abs/2006.04768): a linear approximation of the self-attention mechanism 

## Models
- [Language Models are Few-Shot Learners](https://arxiv.org/abs/2005.14165): the GPT-3 paper, a massive autoregressive language model with 170 billions parameters. 
